{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Abhinav\n",
      "[nltk_data]     Jindal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f4a32bd850>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import pickle\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import contractions\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "torch.manual_seed(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Reading data from data.tsv file in the current diectory, using separator as \\t and skipping bad data lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav Jindal\\AppData\\Local\\Temp\\ipykernel_4396\\3209690956.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_dataset = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = pd.read_csv(\n",
    "     './data.tsv', \n",
    "     sep='\\t',\n",
    "     on_bad_lines='skip'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep reviews and star rating\n",
    "\n",
    "only keeping the \"review body\" and \"star rating\" columns in the read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav Jindal\\AppData\\Local\\Temp\\ipykernel_4396\\3482623344.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_dataset['review_body'] = filtered_dataset['review_body'].astype('str', errors='ignore')\n",
      "C:\\Users\\Abhinav Jindal\\AppData\\Local\\Temp\\ipykernel_4396\\3482623344.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_dataset['star_rating'] = filtered_dataset['star_rating'].astype('int64', errors='ignore')\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset = raw_dataset[['review_body', 'star_rating']]\n",
    "filtered_dataset['review_body'] = filtered_dataset['review_body'].astype('str', errors='ignore')\n",
    "filtered_dataset['star_rating'] = filtered_dataset['star_rating'].astype('int64', errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n",
    "We create 3 classes similar to HW1 where ratings <= 2 and given class 1, greater than equal to 4 are given class3 and rest are given class 2. We then sample 20000 random samples from each class to create our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav Jindal\\AppData\\Local\\Temp\\ipykernel_4396\\808810635.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_dataset['class'] = filtered_dataset['star_rating'].map(class_assign)\n"
     ]
    }
   ],
   "source": [
    "def class_assign(value):\n",
    "    try:\n",
    "        value = float(value)\n",
    "        if value <= 2:\n",
    "            return 1\n",
    "        elif value >= 4:\n",
    "            return 3\n",
    "        else:\n",
    "            return 2\n",
    "    except Exception as e:\n",
    "        return 4\n",
    " \n",
    "filtered_dataset['class'] = filtered_dataset['star_rating'].map(class_assign)\n",
    "df1 = filtered_dataset.loc[filtered_dataset['class'] == 1].sample(20000, random_state=30)\n",
    "df2 = filtered_dataset.loc[filtered_dataset['class'] == 2].sample(20000, random_state=30)\n",
    "df3 = filtered_dataset.loc[filtered_dataset['class'] == 3].sample(20000, random_state=30)\n",
    "dataset = pd.concat([df1, df2, df3])\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used pickle for writing and reading some objects to reduce some computational load while testing and finetuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)                \n",
    "        f.close()\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        obj = pickle.load(f)                     \n",
    "        f.close()\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write and read the dataset from pickle for faster computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle(dataset, './dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great product</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My wife received this hair dryer as a gift a f...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do you compare the hundreds of skin care p...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Every well groomed guy needs one of these! My ...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy is this stuff hot!</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>I love this nail polish! All you do is put on ...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>I am allergic to everything; however, I can us...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>So far I haven't seen anything about this eye ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>Not worth the money for 100% plastic. No linin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>great product will order more in the future</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             review_body star_rating  class\n",
       "0                                          great product         5.0      3\n",
       "1      My wife received this hair dryer as a gift a f...           2      1\n",
       "2      How do you compare the hundreds of skin care p...           4      3\n",
       "3      Every well groomed guy needs one of these! My ...           5      3\n",
       "4                                 Boy is this stuff hot!           5      3\n",
       "...                                                  ...         ...    ...\n",
       "59995  I love this nail polish! All you do is put on ...           5      3\n",
       "59996  I am allergic to everything; however, I can us...           5      3\n",
       "59997  So far I haven't seen anything about this eye ...           3      2\n",
       "59998  Not worth the money for 100% plastic. No linin...           1      1\n",
       "59999        great product will order more in the future           5      3\n",
       "\n",
       "[60000 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_pickle('./dataset.pkl')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review_body(s):\n",
    "    s = s.lower()\n",
    "    # remove html tags\n",
    "    s = re.sub(r'<[^<]+?>', ' ', s)\n",
    "    # remove urls\n",
    "    s = re.sub(r'http\\S+', ' ', s)\n",
    "    s = re.sub(r'www\\S+', ' ', s)\n",
    "    # remove extra spaces\n",
    "    s = re.sub(r\"\\s+\", ' ', s)\n",
    "    # fix contractions\n",
    "    s = contractions.fix(s)\n",
    "    # remove non word characters\n",
    "    s = re.sub(r\"[^a-z\\s]\", ' ', s)\n",
    "    return s\n",
    "\n",
    "dataset['review_body'] = dataset['review_body'].map(clean_review_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): word2vec-google-news-300\n",
    "\n",
    "Loaded the google news 300 word2vec model and created 2 helper functions, one to get the vec for a word and handle exceptions if any and, other to get similarity score between 2 vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # reading the model from pickle if it's already ran once and saved using pickle\n",
    "    google_model = load_pickle('./google_model.pkl')\n",
    "except Exception as e:\n",
    "    # loading the google news word2vec model  and saving it using pickle if not already present to improve computational load\n",
    "    google_model = api.load('word2vec-google-news-300')\n",
    "    write_pickle(google_model, './google_model.pkl')\n",
    "\n",
    "def get_word_to_vec(word, model):\n",
    "    try:\n",
    "        vec = model[word]\n",
    "    except KeyError:\n",
    "        print(\"The word '{}' does not appear in this model\".format(word))\n",
    "        vec = None\n",
    "    return vec\n",
    "\n",
    "def get_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1)* np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google word2vec similarity scores\n",
      "Similarity between queen and (king - man + woman): 0.73005176\n",
      "Similarity between jacket and coat: 0.6492949\n",
      "Similarity between king and france: 0.16112953\n"
     ]
    }
   ],
   "source": [
    "king_vec = get_word_to_vec('king', google_model)\n",
    "man_vec = get_word_to_vec('man', google_model)\n",
    "woman_vec = get_word_to_vec('woman', google_model)\n",
    "queen_vec = get_word_to_vec('queen', google_model)\n",
    "approx_queen_vec = king_vec - man_vec + woman_vec\n",
    "print(\"Google word2vec similarity scores\")\n",
    "print (\"Similarity between queen and (king - man + woman):\", get_similarity(queen_vec, approx_queen_vec))\n",
    "print (\"Similarity between jacket and coat:\", google_model.similarity('jacket', 'coat'))\n",
    "print (\"Similarity between king and france:\", google_model.similarity('king', 'france'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Train word2vec using own dataset\n",
    "\n",
    "training custom word2vec model using our dataset with window size 13 and vector size 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## applying preprocessing provided by gensim library\n",
    "review_text = dataset['review_body'].apply(gensim.utils.simple_preprocess)\n",
    "model = gensim.models.Word2Vec(\n",
    "    window=13,\n",
    "    vector_size=300,\n",
    "    min_count=9,\n",
    "    workers=4,\n",
    ")\n",
    "model.build_vocab(review_text)\n",
    "model.train(review_text, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save(\"./word2vec-custom.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our custom word embedding model similarity scores\n",
      "Similarity between queen and (king - man + woman): 0.32972342\n",
      "Similarity between jacket and coat: 0.041718163\n",
      "Similarity between king and france: 0.57025325\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(\"./word2vec-custom.model\")\n",
    "king_vec = get_word_to_vec('king', model.wv)\n",
    "man_vec = get_word_to_vec('man', model.wv)\n",
    "woman_vec = get_word_to_vec('woman', model.wv)\n",
    "queen_vec = get_word_to_vec('queen', model.wv)\n",
    "approx_queen_vec = king_vec - man_vec + woman_vec\n",
    "print(\"Our custom word embedding model similarity scores\")\n",
    "print (\"Similarity between queen and (king - man + woman):\", get_similarity(queen_vec, approx_queen_vec))\n",
    "print (\"Similarity between jacket and coat:\", model.wv.similarity('jacket', 'coat'))\n",
    "print (\"Similarity between king and france:\", model.wv.similarity('king', 'france'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The embeddings generated by the pretrained google word2vec seems to be much better compared to the model trained on our own dataset.  The pretrained google model encodes semantic similarities between words better.  This is probably because of the difference in the amount of data used to train these models. Google model is trained on a large dataset while our model just uses 60k reviews.\n",
    "\n",
    "##### From our examples we see, it assigns higher score of 0.73 to similarity between woman and (king-man+woman) compared to 0.33 in our custom dataset. Also, jacket and coat are given a similarity score of 0.65 in google model whereas our model gives it a small score of 0.04 even though jacket and coat are highly similar and are often used in the same context of clothes. Similarly for very dissimilar words like king and france, google model gives a lower score of 0.16 and our model gives 0.57 which is not appropriate as these 2 words are not that highly related and are rarely used in the same context. Hence, google pretrained model returns much better encoding compared to our model. This could be primarily because google model is trained on a very large dataset compared to our model which is just trained on 60k reviews which is not a lot of data for a good word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we use gensims simple preprocess to do further minor preprocessing already provided by gensim to improve our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['review_preprocessed_tokens'] = dataset['review_body'].apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we calculate the mean vectors from the embeddings by google model, if none of the words are present in the google model we return a vector of zeros of size 300\n",
    "\n",
    "#### we then split the dataset and train models on our training dataset and test on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector(review_preprocessed_tokens):\n",
    "    vectors = [google_model[word] for word in review_preprocessed_tokens if word in google_model]\n",
    "    if len(vectors) > 0:\n",
    "        feature_vec = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        feature_vec = np.zeros(300)\n",
    "    return feature_vec\n",
    "\n",
    "dataset['reviews_vector'] = dataset['review_preprocessed_tokens'].map(mean_vector)\n",
    "finished_dataset = dataset[['reviews_vector', 'class']]\n",
    "\n",
    "training_data, testing_data = train_test_split(finished_dataset, test_size=0.2, random_state=25)\n",
    "\n",
    "train_X = np.stack(training_data['reviews_vector'])\n",
    "train_Y = np.array(training_data['class'])\n",
    "test_X = np.stack(testing_data['reviews_vector'])\n",
    "test_Y =np.array(testing_data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for word2vec perceptron: 0.59025\n"
     ]
    }
   ],
   "source": [
    "clf = Perceptron(penalty='elasticnet', alpha=0.00001, tol=1e-7, random_state=10)\n",
    "clf.fit(train_X, train_Y)\n",
    "print (\"Accuracy for word2vec perceptron:\", accuracy_score(test_Y, clf.predict(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy using tf-idf for perceptron (calculated from HW1): 0.6474166666666666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for word2vec SVM: 0.6615833333333333\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(penalty='l2', loss='squared_hinge', tol=1e-7, dual=True, random_state=25)\n",
    "clf.fit(train_X, train_Y)\n",
    "print (\"Accuracy for word2vec SVM:\", accuracy_score(test_Y, clf.predict(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy using tf-idf for SVM (calculated from HW1): 0.7070833333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that tf-idf performs better for these 2 models, perceptron and SVM when compared with word2vec. This could be because word2vec vectors have complicated relations which are not easily captured by these simple models. Also, taking mean of the word2vec vectors might lead to loss of information which could be another cause of lesser accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change to cuda if want to use GPU\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move data to tensors and appropriate device and also change label to torch conventions\n",
    "def format_data_for_model(train_X, train_Y, test_X, test_Y, device):\n",
    "    X_train = torch.tensor(train_X).to(torch.float)\n",
    "    X_test = torch.tensor(test_X).to(torch.float)\n",
    "    Y_train = torch.tensor(train_Y).long()\n",
    "    Y_test = torch.tensor(test_Y).long()\n",
    "    X_train = X_train.to(device)\n",
    "    Y_train = Y_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    Y_test = Y_test.to(device)\n",
    "    # move labels from 1,2,3 to 0,1,2 to fit to pytorch conventions\n",
    "    Y_train = Y_train - 1\n",
    "    Y_test = Y_test - 1\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trains model using parameters passed and prints train and test loss and accuracy after each epoch\n",
    "def train_model(model, optimizer, loss_fn, X_train, Y_train, X_test, Y_test, num_epochs, batch_size):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Shuffle the training data\n",
    "        indices = torch.randperm(X_train.shape[0])\n",
    "        x_train = X_train[indices]\n",
    "        y_train = Y_train[indices]\n",
    "\n",
    "        # batch for training data\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            X_batch = x_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "            # Compute the forward pass through the network\n",
    "            y_pred = model(X_batch)                \n",
    "            \n",
    "            ## zero out the gradient, calcualate loss and back propagate loss\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate performance after each epoch\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            # train loss and accuracy\n",
    "            y_pred = model(X_train)\n",
    "            train_loss = loss_fn(y_pred, Y_train)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            train_accuracy = (y_pred == Y_train).float().mean()\n",
    "\n",
    "            # test loss and accuracy\n",
    "            y_pred = model(X_test)\n",
    "            test_loss = loss_fn(y_pred, Y_test)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            test_accuracy = (y_pred == Y_test).float().mean()\n",
    "\n",
    "        # Print the epoch, loss, and accuracy\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4(a)  MLP using average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to use the same mean vectors data we used for simple models, hence no separate preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = format_data_for_model(train_X, train_Y, test_X, test_Y, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create a MLP model class with 2 hidden layers (100 and 10 nodes each) and use Relu as the activation function. We also use SGD optimizer and CrossEntropyLoss as this was giving the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create the MLP model and optimizer\n",
    "model = MLP()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# As the loss is CrossEntropy we don't need to add softmax separately\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the following hyperparameters: <br/>\n",
    "learning rate = 0.01 <br/>\n",
    "loss function = CrossEntropyLoss <br/>\n",
    "optimizer = SGD <br/>\n",
    "momentum = 0.9 <br/>\n",
    "non-linear activation = Relu <br/>\n",
    "epochs = 50 <br/>\n",
    "batch size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.8410, Train Accuracy: 0.6064\n",
      "Epoch 1/50, Test Loss: 0.8459, Test Accuracy: 0.6040\n",
      "Epoch 2/50, Train Loss: 0.7828, Train Accuracy: 0.6531\n",
      "Epoch 2/50, Test Loss: 0.7882, Test Accuracy: 0.6518\n",
      "Epoch 3/50, Train Loss: 0.7750, Train Accuracy: 0.6447\n",
      "Epoch 3/50, Test Loss: 0.7869, Test Accuracy: 0.6363\n",
      "Epoch 4/50, Train Loss: 0.7711, Train Accuracy: 0.6522\n",
      "Epoch 4/50, Test Loss: 0.7830, Test Accuracy: 0.6495\n",
      "Epoch 5/50, Train Loss: 0.7504, Train Accuracy: 0.6685\n",
      "Epoch 5/50, Test Loss: 0.7641, Test Accuracy: 0.6602\n",
      "Epoch 6/50, Train Loss: 0.7992, Train Accuracy: 0.6382\n",
      "Epoch 6/50, Test Loss: 0.8207, Test Accuracy: 0.6277\n",
      "Epoch 7/50, Train Loss: 0.7520, Train Accuracy: 0.6643\n",
      "Epoch 7/50, Test Loss: 0.7749, Test Accuracy: 0.6511\n",
      "Epoch 8/50, Train Loss: 0.7308, Train Accuracy: 0.6803\n",
      "Epoch 8/50, Test Loss: 0.7498, Test Accuracy: 0.6712\n",
      "Epoch 9/50, Train Loss: 0.7390, Train Accuracy: 0.6708\n",
      "Epoch 9/50, Test Loss: 0.7643, Test Accuracy: 0.6584\n",
      "Epoch 10/50, Train Loss: 0.7477, Train Accuracy: 0.6601\n",
      "Epoch 10/50, Test Loss: 0.7736, Test Accuracy: 0.6478\n",
      "Epoch 11/50, Train Loss: 0.7281, Train Accuracy: 0.6793\n",
      "Epoch 11/50, Test Loss: 0.7516, Test Accuracy: 0.6707\n",
      "Epoch 12/50, Train Loss: 0.7332, Train Accuracy: 0.6776\n",
      "Epoch 12/50, Test Loss: 0.7617, Test Accuracy: 0.6631\n",
      "Epoch 13/50, Train Loss: 0.7237, Train Accuracy: 0.6810\n",
      "Epoch 13/50, Test Loss: 0.7488, Test Accuracy: 0.6700\n",
      "Epoch 14/50, Train Loss: 0.7064, Train Accuracy: 0.6919\n",
      "Epoch 14/50, Test Loss: 0.7378, Test Accuracy: 0.6760\n",
      "Epoch 15/50, Train Loss: 0.7147, Train Accuracy: 0.6859\n",
      "Epoch 15/50, Test Loss: 0.7464, Test Accuracy: 0.6704\n",
      "Epoch 16/50, Train Loss: 0.7069, Train Accuracy: 0.6835\n",
      "Epoch 16/50, Test Loss: 0.7428, Test Accuracy: 0.6654\n",
      "Epoch 17/50, Train Loss: 0.7042, Train Accuracy: 0.6893\n",
      "Epoch 17/50, Test Loss: 0.7384, Test Accuracy: 0.6745\n",
      "Epoch 18/50, Train Loss: 0.7013, Train Accuracy: 0.6924\n",
      "Epoch 18/50, Test Loss: 0.7379, Test Accuracy: 0.6733\n",
      "Epoch 19/50, Train Loss: 0.6911, Train Accuracy: 0.6968\n",
      "Epoch 19/50, Test Loss: 0.7306, Test Accuracy: 0.6766\n",
      "Epoch 20/50, Train Loss: 0.6982, Train Accuracy: 0.6921\n",
      "Epoch 20/50, Test Loss: 0.7394, Test Accuracy: 0.6740\n",
      "Epoch 21/50, Train Loss: 0.7192, Train Accuracy: 0.6808\n",
      "Epoch 21/50, Test Loss: 0.7638, Test Accuracy: 0.6562\n",
      "Epoch 22/50, Train Loss: 0.6926, Train Accuracy: 0.6948\n",
      "Epoch 22/50, Test Loss: 0.7457, Test Accuracy: 0.6712\n",
      "Epoch 23/50, Train Loss: 0.7041, Train Accuracy: 0.6872\n",
      "Epoch 23/50, Test Loss: 0.7520, Test Accuracy: 0.6676\n",
      "Epoch 24/50, Train Loss: 0.7447, Train Accuracy: 0.6683\n",
      "Epoch 24/50, Test Loss: 0.7964, Test Accuracy: 0.6407\n",
      "Epoch 25/50, Train Loss: 0.6775, Train Accuracy: 0.7017\n",
      "Epoch 25/50, Test Loss: 0.7340, Test Accuracy: 0.6743\n",
      "Epoch 26/50, Train Loss: 0.6737, Train Accuracy: 0.7056\n",
      "Epoch 26/50, Test Loss: 0.7292, Test Accuracy: 0.6795\n",
      "Epoch 27/50, Train Loss: 0.6877, Train Accuracy: 0.6955\n",
      "Epoch 27/50, Test Loss: 0.7388, Test Accuracy: 0.6689\n",
      "Epoch 28/50, Train Loss: 0.6718, Train Accuracy: 0.7045\n",
      "Epoch 28/50, Test Loss: 0.7421, Test Accuracy: 0.6776\n",
      "Epoch 29/50, Train Loss: 0.6977, Train Accuracy: 0.6846\n",
      "Epoch 29/50, Test Loss: 0.7685, Test Accuracy: 0.6573\n",
      "Epoch 30/50, Train Loss: 0.6638, Train Accuracy: 0.7073\n",
      "Epoch 30/50, Test Loss: 0.7310, Test Accuracy: 0.6769\n",
      "Epoch 31/50, Train Loss: 0.6658, Train Accuracy: 0.7095\n",
      "Epoch 31/50, Test Loss: 0.7342, Test Accuracy: 0.6798\n",
      "Epoch 32/50, Train Loss: 0.6752, Train Accuracy: 0.7005\n",
      "Epoch 32/50, Test Loss: 0.7458, Test Accuracy: 0.6689\n",
      "Epoch 33/50, Train Loss: 0.6651, Train Accuracy: 0.7060\n",
      "Epoch 33/50, Test Loss: 0.7458, Test Accuracy: 0.6731\n",
      "Epoch 34/50, Train Loss: 0.6537, Train Accuracy: 0.7098\n",
      "Epoch 34/50, Test Loss: 0.7402, Test Accuracy: 0.6736\n",
      "Epoch 35/50, Train Loss: 0.6460, Train Accuracy: 0.7179\n",
      "Epoch 35/50, Test Loss: 0.7341, Test Accuracy: 0.6840\n",
      "Epoch 36/50, Train Loss: 0.6586, Train Accuracy: 0.7109\n",
      "Epoch 36/50, Test Loss: 0.7426, Test Accuracy: 0.6777\n",
      "Epoch 37/50, Train Loss: 0.6675, Train Accuracy: 0.7039\n",
      "Epoch 37/50, Test Loss: 0.7528, Test Accuracy: 0.6672\n",
      "Epoch 38/50, Train Loss: 0.6626, Train Accuracy: 0.7060\n",
      "Epoch 38/50, Test Loss: 0.7577, Test Accuracy: 0.6618\n",
      "Epoch 39/50, Train Loss: 0.6457, Train Accuracy: 0.7171\n",
      "Epoch 39/50, Test Loss: 0.7381, Test Accuracy: 0.6722\n",
      "Epoch 40/50, Train Loss: 0.6516, Train Accuracy: 0.7149\n",
      "Epoch 40/50, Test Loss: 0.7460, Test Accuracy: 0.6733\n",
      "Epoch 41/50, Train Loss: 0.6393, Train Accuracy: 0.7209\n",
      "Epoch 41/50, Test Loss: 0.7388, Test Accuracy: 0.6781\n",
      "Epoch 42/50, Train Loss: 0.6405, Train Accuracy: 0.7161\n",
      "Epoch 42/50, Test Loss: 0.7459, Test Accuracy: 0.6715\n",
      "Epoch 43/50, Train Loss: 0.6360, Train Accuracy: 0.7222\n",
      "Epoch 43/50, Test Loss: 0.7388, Test Accuracy: 0.6740\n",
      "Epoch 44/50, Train Loss: 0.6328, Train Accuracy: 0.7230\n",
      "Epoch 44/50, Test Loss: 0.7416, Test Accuracy: 0.6756\n",
      "Epoch 45/50, Train Loss: 0.6431, Train Accuracy: 0.7183\n",
      "Epoch 45/50, Test Loss: 0.7555, Test Accuracy: 0.6722\n",
      "Epoch 46/50, Train Loss: 0.6260, Train Accuracy: 0.7267\n",
      "Epoch 46/50, Test Loss: 0.7488, Test Accuracy: 0.6752\n",
      "Epoch 47/50, Train Loss: 0.6395, Train Accuracy: 0.7208\n",
      "Epoch 47/50, Test Loss: 0.7556, Test Accuracy: 0.6716\n",
      "Epoch 48/50, Train Loss: 0.6186, Train Accuracy: 0.7310\n",
      "Epoch 48/50, Test Loss: 0.7424, Test Accuracy: 0.6786\n",
      "Epoch 49/50, Train Loss: 0.6353, Train Accuracy: 0.7216\n",
      "Epoch 49/50, Test Loss: 0.7617, Test Accuracy: 0.6672\n",
      "Epoch 50/50, Train Loss: 0.6133, Train Accuracy: 0.7333\n",
      "Epoch 50/50, Test Loss: 0.7495, Test Accuracy: 0.6718\n"
     ]
    }
   ],
   "source": [
    "## trains the model and prints accuracy after each epoch\n",
    "model = train_model(model, optimizer, loss_fn, X_train, Y_train, X_test, Y_test, num_epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MLP for mean word2vec: 0.6717\n"
     ]
    }
   ],
   "source": [
    "print (f\"Accuracy for MLP for mean word2vec: {accuracy_score(Y_test, torch.argmax(model(X_test), dim=1)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deleting these variables to improve memory load\n",
    "del model, train_X, train_Y, test_X, test_Y, X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4(b)  concatenate the first 10 Word2Vec vectors for each review as the input feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For concatenation we use ignore the words that are not present in google model and if the number of valid words are less than 10, we pad the rest of the vector with zeros\n",
    "\n",
    "#### We create a MLP model class with 2 hidden layers (100 and 10 nodes each) and use Relu as the activation function. We also use SGD optimizer and CrossEntropyLoss as this was giving the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenated_vector(words):\n",
    "    num_vectors = 0\n",
    "    i = 0\n",
    "    vector = np.empty((10*300))\n",
    "    while num_vectors < 10:\n",
    "        if i < len(words):\n",
    "            current_vector = google_model[words[i]] if words[i] in google_model else None\n",
    "            i += 1\n",
    "            if current_vector is None:\n",
    "                continue\n",
    "        else:\n",
    "            current_vector = np.zeros((300))\n",
    "        vector[num_vectors*300: (num_vectors+1)*300] = current_vector\n",
    "        num_vectors += 1\n",
    "    return vector\n",
    "\n",
    "dataset['reviews_vector'] = dataset['review_preprocessed_tokens'].map(concatenated_vector)\n",
    "finished_dataset = dataset[['reviews_vector', 'class']]\n",
    "\n",
    "training_data, testing_data = train_test_split(finished_dataset, test_size=0.2, random_state=25)\n",
    "\n",
    "train_X = np.stack(training_data['reviews_vector'])\n",
    "train_Y = np.array(training_data['class'])\n",
    "test_X = np.stack(testing_data['reviews_vector'])\n",
    "test_Y = np.array(testing_data['class'])\n",
    "\n",
    "## have data as tensors\n",
    "X_train, X_test, Y_train, Y_test = format_data_for_model(train_X, train_Y, test_X, test_Y, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the multilayer perceptron network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(300*10, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create the MLP model, optimizer and loss function\n",
    "model = MLP()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the following hyperparameters: <br/>\n",
    "learning rate = 0.001 <br/>\n",
    "loss function = CrossEntropyLoss <br/>\n",
    "optimizer = SGD <br/>\n",
    "momentum = 0.9 <br/>\n",
    "non-linear activation = Relu <br/>\n",
    "epochs = 20 <br/>\n",
    "batch size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 1.0758, Train Accuracy: 0.4897\n",
      "Epoch 1/20, Test Loss: 1.0760, Test Accuracy: 0.4869\n",
      "Epoch 2/20, Train Loss: 0.9562, Train Accuracy: 0.5220\n",
      "Epoch 2/20, Test Loss: 0.9591, Test Accuracy: 0.5193\n",
      "Epoch 3/20, Train Loss: 0.9066, Train Accuracy: 0.5654\n",
      "Epoch 3/20, Test Loss: 0.9209, Test Accuracy: 0.5518\n",
      "Epoch 4/20, Train Loss: 0.8707, Train Accuracy: 0.5950\n",
      "Epoch 4/20, Test Loss: 0.8978, Test Accuracy: 0.5739\n",
      "Epoch 5/20, Train Loss: 0.8460, Train Accuracy: 0.6092\n",
      "Epoch 5/20, Test Loss: 0.8853, Test Accuracy: 0.5823\n",
      "Epoch 6/20, Train Loss: 0.8235, Train Accuracy: 0.6217\n",
      "Epoch 6/20, Test Loss: 0.8747, Test Accuracy: 0.5918\n",
      "Epoch 7/20, Train Loss: 0.8080, Train Accuracy: 0.6319\n",
      "Epoch 7/20, Test Loss: 0.8720, Test Accuracy: 0.5928\n",
      "Epoch 8/20, Train Loss: 0.7924, Train Accuracy: 0.6418\n",
      "Epoch 8/20, Test Loss: 0.8683, Test Accuracy: 0.5901\n",
      "Epoch 9/20, Train Loss: 0.7752, Train Accuracy: 0.6497\n",
      "Epoch 9/20, Test Loss: 0.8674, Test Accuracy: 0.5932\n",
      "Epoch 10/20, Train Loss: 0.7629, Train Accuracy: 0.6561\n",
      "Epoch 10/20, Test Loss: 0.8707, Test Accuracy: 0.5908\n",
      "Epoch 11/20, Train Loss: 0.7419, Train Accuracy: 0.6706\n",
      "Epoch 11/20, Test Loss: 0.8692, Test Accuracy: 0.5888\n",
      "Epoch 12/20, Train Loss: 0.7181, Train Accuracy: 0.6873\n",
      "Epoch 12/20, Test Loss: 0.8706, Test Accuracy: 0.5902\n",
      "Epoch 13/20, Train Loss: 0.6974, Train Accuracy: 0.6973\n",
      "Epoch 13/20, Test Loss: 0.8746, Test Accuracy: 0.5900\n",
      "Epoch 14/20, Train Loss: 0.6665, Train Accuracy: 0.7203\n",
      "Epoch 14/20, Test Loss: 0.8763, Test Accuracy: 0.5907\n",
      "Epoch 15/20, Train Loss: 0.6334, Train Accuracy: 0.7403\n",
      "Epoch 15/20, Test Loss: 0.8833, Test Accuracy: 0.5901\n",
      "Epoch 16/20, Train Loss: 0.6049, Train Accuracy: 0.7559\n",
      "Epoch 16/20, Test Loss: 0.9073, Test Accuracy: 0.5838\n",
      "Epoch 17/20, Train Loss: 0.5582, Train Accuracy: 0.7849\n",
      "Epoch 17/20, Test Loss: 0.9148, Test Accuracy: 0.5862\n",
      "Epoch 18/20, Train Loss: 0.5090, Train Accuracy: 0.8131\n",
      "Epoch 18/20, Test Loss: 0.9319, Test Accuracy: 0.5861\n",
      "Epoch 19/20, Train Loss: 0.4641, Train Accuracy: 0.8350\n",
      "Epoch 19/20, Test Loss: 0.9738, Test Accuracy: 0.5779\n",
      "Epoch 20/20, Train Loss: 0.4054, Train Accuracy: 0.8694\n",
      "Epoch 20/20, Test Loss: 0.9961, Test Accuracy: 0.5791\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, loss_fn, X_train, Y_train, X_test, Y_test, num_epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MLP for concatenated word2vec: 0.5791\n"
     ]
    }
   ],
   "source": [
    "print (f\"Accuracy for MLP for concatenated word2vec: {accuracy_score(Y_test, torch.argmax(model(X_test), dim=1)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy using mean word2vec is greater than concatenated word2vec on testing set. This could be because we are just considering the first 10 words in a review which might not contain a lot of useful information  many times.\n",
    "\n",
    "#### When compared with simple models with word2vec embeddings (Task 3), we see that MLP with mean vectors performs better than Perceptron and SVM. This is probably because it is able to encode complex information in the vectors more effectively when comapred to the simple models. MLP with concatenated vectors still performs worse. This is probably because of the same reason as mentioned above that taking first 10 words might be leading to loss of information present in the rest of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For serial vectors, we consider the first 20 valid words present in google model in the review and pad the remaining vectors (if 20 valid vectors are not found) with zeros. Thus we get a 20*300 input vector for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_vector(words):\n",
    "    num_vectors = 0\n",
    "    i = 0\n",
    "    vector = np.empty((20,300))\n",
    "    while num_vectors < 20:\n",
    "        if i < len(words):\n",
    "            current_vector = google_model[words[i]] if words[i] in google_model else None\n",
    "            i += 1\n",
    "            if current_vector is None:\n",
    "                continue\n",
    "        else:\n",
    "            current_vector = np.zeros((300))\n",
    "        vector[num_vectors] = current_vector\n",
    "        num_vectors += 1\n",
    "    return vector\n",
    "\n",
    "dataset['reviews_vector'] = dataset['review_preprocessed_tokens'].map(series_vector)\n",
    "finished_dataset = dataset[['reviews_vector', 'class']]\n",
    "\n",
    "training_data, testing_data = train_test_split(finished_dataset, test_size=0.2, random_state=25)\n",
    "\n",
    "train_X = np.stack(training_data['reviews_vector'])\n",
    "train_Y = np.array(training_data['class'])\n",
    "test_X = np.stack(testing_data['reviews_vector'])\n",
    "test_Y =np.array(testing_data['class'])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = format_data_for_model(train_X, train_Y, test_X, test_Y, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (a) RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create a RNN model class with a hidden layer of size 20. The output of the hidden layer (20 outputs from each series) is averaged before passing to the FC later during forward pass. We also use SGD optimizer and CrossEntropyLoss as this was giving the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(300, 20, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        output, hidden = self.rnn(text)\n",
    "        output = output.mean(dim=1)\n",
    "        fc_output = self.fc(output)\n",
    "        return fc_output\n",
    "\n",
    "model = RNN(300, 20, 3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the following hyperparameters: <br/>\n",
    "learning rate = 0.01 <br/>\n",
    "loss function = CrossEntropyLoss <br/>\n",
    "optimizer = SGD <br/>\n",
    "momentum = 0.9 <br/>\n",
    "epochs = 40 <br/>\n",
    "batch size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Train Loss: 0.9357, Train Accuracy: 0.5101\n",
      "Epoch 1/40, Test Loss: 0.9421, Test Accuracy: 0.5083\n",
      "Epoch 2/40, Train Loss: 0.8597, Train Accuracy: 0.6046\n",
      "Epoch 2/40, Test Loss: 0.8688, Test Accuracy: 0.6024\n",
      "Epoch 3/40, Train Loss: 0.8344, Train Accuracy: 0.6231\n",
      "Epoch 3/40, Test Loss: 0.8428, Test Accuracy: 0.6167\n",
      "Epoch 4/40, Train Loss: 0.8351, Train Accuracy: 0.6244\n",
      "Epoch 4/40, Test Loss: 0.8460, Test Accuracy: 0.6168\n",
      "Epoch 5/40, Train Loss: 0.8035, Train Accuracy: 0.6359\n",
      "Epoch 5/40, Test Loss: 0.8181, Test Accuracy: 0.6252\n",
      "Epoch 6/40, Train Loss: 0.7936, Train Accuracy: 0.6407\n",
      "Epoch 6/40, Test Loss: 0.8079, Test Accuracy: 0.6327\n",
      "Epoch 7/40, Train Loss: 0.8058, Train Accuracy: 0.6293\n",
      "Epoch 7/40, Test Loss: 0.8182, Test Accuracy: 0.6198\n",
      "Epoch 8/40, Train Loss: 0.8001, Train Accuracy: 0.6373\n",
      "Epoch 8/40, Test Loss: 0.8203, Test Accuracy: 0.6289\n",
      "Epoch 9/40, Train Loss: 0.7878, Train Accuracy: 0.6453\n",
      "Epoch 9/40, Test Loss: 0.8127, Test Accuracy: 0.6347\n",
      "Epoch 10/40, Train Loss: 0.7734, Train Accuracy: 0.6490\n",
      "Epoch 10/40, Test Loss: 0.7962, Test Accuracy: 0.6344\n",
      "Epoch 11/40, Train Loss: 0.7729, Train Accuracy: 0.6487\n",
      "Epoch 11/40, Test Loss: 0.8017, Test Accuracy: 0.6332\n",
      "Epoch 12/40, Train Loss: 0.7897, Train Accuracy: 0.6391\n",
      "Epoch 12/40, Test Loss: 0.8124, Test Accuracy: 0.6258\n",
      "Epoch 13/40, Train Loss: 0.7773, Train Accuracy: 0.6530\n",
      "Epoch 13/40, Test Loss: 0.8089, Test Accuracy: 0.6396\n",
      "Epoch 14/40, Train Loss: 0.7583, Train Accuracy: 0.6582\n",
      "Epoch 14/40, Test Loss: 0.7925, Test Accuracy: 0.6426\n",
      "Epoch 15/40, Train Loss: 0.7506, Train Accuracy: 0.6612\n",
      "Epoch 15/40, Test Loss: 0.7879, Test Accuracy: 0.6446\n",
      "Epoch 16/40, Train Loss: 0.7706, Train Accuracy: 0.6547\n",
      "Epoch 16/40, Test Loss: 0.7915, Test Accuracy: 0.6400\n",
      "Epoch 17/40, Train Loss: 0.7929, Train Accuracy: 0.6436\n",
      "Epoch 17/40, Test Loss: 0.8260, Test Accuracy: 0.6310\n",
      "Epoch 18/40, Train Loss: 0.7553, Train Accuracy: 0.6660\n",
      "Epoch 18/40, Test Loss: 0.7836, Test Accuracy: 0.6476\n",
      "Epoch 19/40, Train Loss: 0.7594, Train Accuracy: 0.6554\n",
      "Epoch 19/40, Test Loss: 0.7981, Test Accuracy: 0.6384\n",
      "Epoch 20/40, Train Loss: 0.7437, Train Accuracy: 0.6653\n",
      "Epoch 20/40, Test Loss: 0.7863, Test Accuracy: 0.6459\n",
      "Epoch 21/40, Train Loss: 0.7369, Train Accuracy: 0.6694\n",
      "Epoch 21/40, Test Loss: 0.7802, Test Accuracy: 0.6467\n",
      "Epoch 22/40, Train Loss: 0.7325, Train Accuracy: 0.6729\n",
      "Epoch 22/40, Test Loss: 0.7749, Test Accuracy: 0.6461\n",
      "Epoch 23/40, Train Loss: 0.7745, Train Accuracy: 0.6490\n",
      "Epoch 23/40, Test Loss: 0.8022, Test Accuracy: 0.6312\n",
      "Epoch 24/40, Train Loss: 0.7304, Train Accuracy: 0.6752\n",
      "Epoch 24/40, Test Loss: 0.7747, Test Accuracy: 0.6500\n",
      "Epoch 25/40, Train Loss: 0.7242, Train Accuracy: 0.6789\n",
      "Epoch 25/40, Test Loss: 0.7753, Test Accuracy: 0.6493\n",
      "Epoch 26/40, Train Loss: 0.7237, Train Accuracy: 0.6782\n",
      "Epoch 26/40, Test Loss: 0.7739, Test Accuracy: 0.6509\n",
      "Epoch 27/40, Train Loss: 0.7262, Train Accuracy: 0.6787\n",
      "Epoch 27/40, Test Loss: 0.7793, Test Accuracy: 0.6506\n",
      "Epoch 28/40, Train Loss: 0.7358, Train Accuracy: 0.6700\n",
      "Epoch 28/40, Test Loss: 0.7921, Test Accuracy: 0.6423\n",
      "Epoch 29/40, Train Loss: 0.7184, Train Accuracy: 0.6827\n",
      "Epoch 29/40, Test Loss: 0.7715, Test Accuracy: 0.6521\n",
      "Epoch 30/40, Train Loss: 0.7167, Train Accuracy: 0.6825\n",
      "Epoch 30/40, Test Loss: 0.7734, Test Accuracy: 0.6515\n",
      "Epoch 31/40, Train Loss: 0.7193, Train Accuracy: 0.6822\n",
      "Epoch 31/40, Test Loss: 0.7716, Test Accuracy: 0.6505\n",
      "Epoch 32/40, Train Loss: 0.7189, Train Accuracy: 0.6774\n",
      "Epoch 32/40, Test Loss: 0.7826, Test Accuracy: 0.6437\n",
      "Epoch 33/40, Train Loss: 0.7264, Train Accuracy: 0.6719\n",
      "Epoch 33/40, Test Loss: 0.7954, Test Accuracy: 0.6405\n",
      "Epoch 34/40, Train Loss: 0.7225, Train Accuracy: 0.6791\n",
      "Epoch 34/40, Test Loss: 0.7816, Test Accuracy: 0.6440\n",
      "Epoch 35/40, Train Loss: 0.7367, Train Accuracy: 0.6646\n",
      "Epoch 35/40, Test Loss: 0.7969, Test Accuracy: 0.6371\n",
      "Epoch 36/40, Train Loss: 0.7136, Train Accuracy: 0.6839\n",
      "Epoch 36/40, Test Loss: 0.7835, Test Accuracy: 0.6495\n",
      "Epoch 37/40, Train Loss: 0.7177, Train Accuracy: 0.6798\n",
      "Epoch 37/40, Test Loss: 0.7987, Test Accuracy: 0.6437\n",
      "Epoch 38/40, Train Loss: 0.7157, Train Accuracy: 0.6812\n",
      "Epoch 38/40, Test Loss: 0.7851, Test Accuracy: 0.6472\n",
      "Epoch 39/40, Train Loss: 0.7066, Train Accuracy: 0.6872\n",
      "Epoch 39/40, Test Loss: 0.7767, Test Accuracy: 0.6519\n",
      "Epoch 40/40, Train Loss: 0.7033, Train Accuracy: 0.6894\n",
      "Epoch 40/40, Test Loss: 0.7757, Test Accuracy: 0.6478\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, loss_fn, X_train, Y_train, X_test, Y_test, num_epochs=40, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for RNN: 0.6478\n"
     ]
    }
   ],
   "source": [
    "print (f\"Accuracy for RNN: {accuracy_score(Y_test, torch.argmax(model(X_test), dim=1)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy is slightly less than MLP with mean word2vec vectors but it is higher than the MLP with concatenated word2vec vectors. This is because in RNN we are considering the first 20 words which have more information than 10 words in concatenated word2vec. Also, MLP with mean vectors might be performing better because of the vanishing gradient problem in RNN and that still some information might be missing for reviews greater than length 20. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (b) GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### same as RNN model but GRU layer instead of RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (rnn): GRU(300, 20, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        output, hidden = self.rnn(text)\n",
    "        output = output.mean(dim=1)\n",
    "        fc_output = self.fc(output)\n",
    "        return fc_output\n",
    "\n",
    "model = GRU(300, 20, 3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the following hyperparameters: <br/>\n",
    "learning rate = 0.01 <br/>\n",
    "loss function = CrossEntropyLoss <br/>\n",
    "optimizer = SGD <br/>\n",
    "momentum = 0.9 <br/>\n",
    "epochs = 30 <br/>\n",
    "batch size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.8834, Train Accuracy: 0.5820\n",
      "Epoch 1/30, Test Loss: 0.8865, Test Accuracy: 0.5762\n",
      "Epoch 2/30, Train Loss: 0.8301, Train Accuracy: 0.6193\n",
      "Epoch 2/30, Test Loss: 0.8340, Test Accuracy: 0.6189\n",
      "Epoch 3/30, Train Loss: 0.8181, Train Accuracy: 0.6231\n",
      "Epoch 3/30, Test Loss: 0.8221, Test Accuracy: 0.6220\n",
      "Epoch 4/30, Train Loss: 0.7996, Train Accuracy: 0.6370\n",
      "Epoch 4/30, Test Loss: 0.8061, Test Accuracy: 0.6348\n",
      "Epoch 5/30, Train Loss: 0.7844, Train Accuracy: 0.6437\n",
      "Epoch 5/30, Test Loss: 0.7954, Test Accuracy: 0.6352\n",
      "Epoch 6/30, Train Loss: 0.7980, Train Accuracy: 0.6357\n",
      "Epoch 6/30, Test Loss: 0.8120, Test Accuracy: 0.6268\n",
      "Epoch 7/30, Train Loss: 0.7885, Train Accuracy: 0.6415\n",
      "Epoch 7/30, Test Loss: 0.8021, Test Accuracy: 0.6342\n",
      "Epoch 8/30, Train Loss: 0.7570, Train Accuracy: 0.6581\n",
      "Epoch 8/30, Test Loss: 0.7743, Test Accuracy: 0.6455\n",
      "Epoch 9/30, Train Loss: 0.7557, Train Accuracy: 0.6581\n",
      "Epoch 9/30, Test Loss: 0.7759, Test Accuracy: 0.6445\n",
      "Epoch 10/30, Train Loss: 0.7443, Train Accuracy: 0.6668\n",
      "Epoch 10/30, Test Loss: 0.7646, Test Accuracy: 0.6575\n",
      "Epoch 11/30, Train Loss: 0.7597, Train Accuracy: 0.6508\n",
      "Epoch 11/30, Test Loss: 0.7807, Test Accuracy: 0.6382\n",
      "Epoch 12/30, Train Loss: 0.7593, Train Accuracy: 0.6556\n",
      "Epoch 12/30, Test Loss: 0.7854, Test Accuracy: 0.6413\n",
      "Epoch 13/30, Train Loss: 0.7342, Train Accuracy: 0.6706\n",
      "Epoch 13/30, Test Loss: 0.7627, Test Accuracy: 0.6534\n",
      "Epoch 14/30, Train Loss: 0.7232, Train Accuracy: 0.6779\n",
      "Epoch 14/30, Test Loss: 0.7520, Test Accuracy: 0.6614\n",
      "Epoch 15/30, Train Loss: 0.7506, Train Accuracy: 0.6617\n",
      "Epoch 15/30, Test Loss: 0.7813, Test Accuracy: 0.6486\n",
      "Epoch 16/30, Train Loss: 0.7148, Train Accuracy: 0.6821\n",
      "Epoch 16/30, Test Loss: 0.7492, Test Accuracy: 0.6654\n",
      "Epoch 17/30, Train Loss: 0.7237, Train Accuracy: 0.6752\n",
      "Epoch 17/30, Test Loss: 0.7609, Test Accuracy: 0.6562\n",
      "Epoch 18/30, Train Loss: 0.7040, Train Accuracy: 0.6869\n",
      "Epoch 18/30, Test Loss: 0.7439, Test Accuracy: 0.6648\n",
      "Epoch 19/30, Train Loss: 0.7029, Train Accuracy: 0.6869\n",
      "Epoch 19/30, Test Loss: 0.7480, Test Accuracy: 0.6628\n",
      "Epoch 20/30, Train Loss: 0.6982, Train Accuracy: 0.6885\n",
      "Epoch 20/30, Test Loss: 0.7476, Test Accuracy: 0.6660\n",
      "Epoch 21/30, Train Loss: 0.6960, Train Accuracy: 0.6900\n",
      "Epoch 21/30, Test Loss: 0.7480, Test Accuracy: 0.6656\n",
      "Epoch 22/30, Train Loss: 0.6872, Train Accuracy: 0.6945\n",
      "Epoch 22/30, Test Loss: 0.7389, Test Accuracy: 0.6657\n",
      "Epoch 23/30, Train Loss: 0.6862, Train Accuracy: 0.6948\n",
      "Epoch 23/30, Test Loss: 0.7395, Test Accuracy: 0.6672\n",
      "Epoch 24/30, Train Loss: 0.6905, Train Accuracy: 0.6928\n",
      "Epoch 24/30, Test Loss: 0.7510, Test Accuracy: 0.6610\n",
      "Epoch 25/30, Train Loss: 0.6849, Train Accuracy: 0.6961\n",
      "Epoch 25/30, Test Loss: 0.7426, Test Accuracy: 0.6626\n",
      "Epoch 26/30, Train Loss: 0.6843, Train Accuracy: 0.6975\n",
      "Epoch 26/30, Test Loss: 0.7469, Test Accuracy: 0.6633\n",
      "Epoch 27/30, Train Loss: 0.6776, Train Accuracy: 0.7001\n",
      "Epoch 27/30, Test Loss: 0.7415, Test Accuracy: 0.6638\n",
      "Epoch 28/30, Train Loss: 0.6729, Train Accuracy: 0.7013\n",
      "Epoch 28/30, Test Loss: 0.7409, Test Accuracy: 0.6666\n",
      "Epoch 29/30, Train Loss: 0.6623, Train Accuracy: 0.7078\n",
      "Epoch 29/30, Test Loss: 0.7348, Test Accuracy: 0.6691\n",
      "Epoch 30/30, Train Loss: 0.6791, Train Accuracy: 0.6980\n",
      "Epoch 30/30, Test Loss: 0.7501, Test Accuracy: 0.6603\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, loss_fn, X_train, Y_train, X_test, Y_test, num_epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for GRU: 0.6603\n"
     ]
    }
   ],
   "source": [
    "print (f\"Accuracy for GRU: {accuracy_score(Y_test, torch.argmax(model(X_test), dim=1)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy is better than simple RNN. It is now comparable to MLP with mean word2vec vectors and much higher than the MLP with concatenated word2vec vectors. Rest of the factors while comparing with MLP remain same as the RNN explanation above, but the vanishing gradient problem is reduced a bit and hence we see some better results than simple RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (b) LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### same as RNN model but LSTM layer instead of RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (rnn): LSTM(300, 20, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        output, hidden = self.rnn(text)\n",
    "        output = output.mean(dim=1)\n",
    "        fc_output = self.fc(output)\n",
    "        return fc_output\n",
    "\n",
    "model = LSTM(300, 20, 3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the following hyperparameters: <br/>\n",
    "learning rate = 0.01 <br/>\n",
    "loss function = CrossEntropyLoss <br/>\n",
    "optimizer = SGD <br/>\n",
    "momentum = 0.9 <br/>\n",
    "epochs = 30 <br/>\n",
    "batch size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.9286, Train Accuracy: 0.5455\n",
      "Epoch 1/30, Test Loss: 0.9339, Test Accuracy: 0.5402\n",
      "Epoch 2/30, Train Loss: 0.8642, Train Accuracy: 0.5926\n",
      "Epoch 2/30, Test Loss: 0.8701, Test Accuracy: 0.5867\n",
      "Epoch 3/30, Train Loss: 0.8247, Train Accuracy: 0.6250\n",
      "Epoch 3/30, Test Loss: 0.8313, Test Accuracy: 0.6184\n",
      "Epoch 4/30, Train Loss: 0.8333, Train Accuracy: 0.6178\n",
      "Epoch 4/30, Test Loss: 0.8407, Test Accuracy: 0.6158\n",
      "Epoch 5/30, Train Loss: 0.7945, Train Accuracy: 0.6382\n",
      "Epoch 5/30, Test Loss: 0.8065, Test Accuracy: 0.6302\n",
      "Epoch 6/30, Train Loss: 0.7854, Train Accuracy: 0.6469\n",
      "Epoch 6/30, Test Loss: 0.7969, Test Accuracy: 0.6381\n",
      "Epoch 7/30, Train Loss: 0.7695, Train Accuracy: 0.6532\n",
      "Epoch 7/30, Test Loss: 0.7849, Test Accuracy: 0.6453\n",
      "Epoch 8/30, Train Loss: 0.7646, Train Accuracy: 0.6533\n",
      "Epoch 8/30, Test Loss: 0.7821, Test Accuracy: 0.6416\n",
      "Epoch 9/30, Train Loss: 0.7702, Train Accuracy: 0.6531\n",
      "Epoch 9/30, Test Loss: 0.7876, Test Accuracy: 0.6408\n",
      "Epoch 10/30, Train Loss: 0.7542, Train Accuracy: 0.6622\n",
      "Epoch 10/30, Test Loss: 0.7747, Test Accuracy: 0.6488\n",
      "Epoch 11/30, Train Loss: 0.7452, Train Accuracy: 0.6633\n",
      "Epoch 11/30, Test Loss: 0.7694, Test Accuracy: 0.6534\n",
      "Epoch 12/30, Train Loss: 0.7487, Train Accuracy: 0.6642\n",
      "Epoch 12/30, Test Loss: 0.7730, Test Accuracy: 0.6488\n",
      "Epoch 13/30, Train Loss: 0.7343, Train Accuracy: 0.6704\n",
      "Epoch 13/30, Test Loss: 0.7635, Test Accuracy: 0.6544\n",
      "Epoch 14/30, Train Loss: 0.7310, Train Accuracy: 0.6719\n",
      "Epoch 14/30, Test Loss: 0.7606, Test Accuracy: 0.6561\n",
      "Epoch 15/30, Train Loss: 0.7317, Train Accuracy: 0.6733\n",
      "Epoch 15/30, Test Loss: 0.7641, Test Accuracy: 0.6548\n",
      "Epoch 16/30, Train Loss: 0.7196, Train Accuracy: 0.6780\n",
      "Epoch 16/30, Test Loss: 0.7567, Test Accuracy: 0.6566\n",
      "Epoch 17/30, Train Loss: 0.7148, Train Accuracy: 0.6806\n",
      "Epoch 17/30, Test Loss: 0.7516, Test Accuracy: 0.6590\n",
      "Epoch 18/30, Train Loss: 0.7228, Train Accuracy: 0.6763\n",
      "Epoch 18/30, Test Loss: 0.7595, Test Accuracy: 0.6543\n",
      "Epoch 19/30, Train Loss: 0.7245, Train Accuracy: 0.6713\n",
      "Epoch 19/30, Test Loss: 0.7687, Test Accuracy: 0.6513\n",
      "Epoch 20/30, Train Loss: 0.7091, Train Accuracy: 0.6814\n",
      "Epoch 20/30, Test Loss: 0.7529, Test Accuracy: 0.6577\n",
      "Epoch 21/30, Train Loss: 0.7037, Train Accuracy: 0.6870\n",
      "Epoch 21/30, Test Loss: 0.7496, Test Accuracy: 0.6584\n",
      "Epoch 22/30, Train Loss: 0.7020, Train Accuracy: 0.6848\n",
      "Epoch 22/30, Test Loss: 0.7514, Test Accuracy: 0.6601\n",
      "Epoch 23/30, Train Loss: 0.6947, Train Accuracy: 0.6909\n",
      "Epoch 23/30, Test Loss: 0.7510, Test Accuracy: 0.6609\n",
      "Epoch 24/30, Train Loss: 0.6916, Train Accuracy: 0.6916\n",
      "Epoch 24/30, Test Loss: 0.7555, Test Accuracy: 0.6614\n",
      "Epoch 25/30, Train Loss: 0.6917, Train Accuracy: 0.6911\n",
      "Epoch 25/30, Test Loss: 0.7458, Test Accuracy: 0.6640\n",
      "Epoch 26/30, Train Loss: 0.7105, Train Accuracy: 0.6799\n",
      "Epoch 26/30, Test Loss: 0.7728, Test Accuracy: 0.6542\n",
      "Epoch 27/30, Train Loss: 0.6784, Train Accuracy: 0.6967\n",
      "Epoch 27/30, Test Loss: 0.7409, Test Accuracy: 0.6645\n",
      "Epoch 28/30, Train Loss: 0.6809, Train Accuracy: 0.6991\n",
      "Epoch 28/30, Test Loss: 0.7516, Test Accuracy: 0.6624\n",
      "Epoch 29/30, Train Loss: 0.6744, Train Accuracy: 0.7000\n",
      "Epoch 29/30, Test Loss: 0.7472, Test Accuracy: 0.6666\n",
      "Epoch 30/30, Train Loss: 0.6692, Train Accuracy: 0.7031\n",
      "Epoch 30/30, Test Loss: 0.7456, Test Accuracy: 0.6638\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, loss_fn, X_train, Y_train, X_test, Y_test, num_epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LSTM: 0.6638\n"
     ]
    }
   ],
   "source": [
    "print (f\"Accuracy for LSTM: {accuracy_score(Y_test, torch.argmax(model(X_test), dim=1)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy is better than simple RNN and slightly better (almost same) than GRU. The comparison with MLP remains the same as RNN as well as the explanations. The vanishing gradient problem is reduced a bit compared to RNN and hence we see some better results than simple RNN. LSTM and GRU give almost similar results as they both handle vanishing gradient in their own way and both seem to performing almost equally in our case. (GRU and LSTM order varies from run to run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU ~= LSTM > Simple RNN.\n",
    "\n",
    "In this case, we see that LSTM performs the best, than GRU and finally simple RNN. This is because RNN faces the problem of vanisihing gradients which both LSTM and GRU try to handle in their own ways. GRU and LSTM both are equally good and their relative order is usually varying from run to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
